{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respiratory-helping",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Inspired from this [article](https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-premiere",
   "metadata": {},
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fallen-uruguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Article Heading First sentence of some important article. And another one. And then the last one'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from bs4 import BeautifulSoup\n",
    "# function to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "# call function\n",
    "remove_html_tags(\" <html> \\\n",
    " <h1>Article Heading</h1> \\\n",
    " <p>First sentence of some important article. And another one. And then the last one</p></html>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-individual",
   "metadata": {},
   "source": [
    "### Removing Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "younger-validation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text. Some words such as resume, cafe, protest, divorce, coordinate, expose, latte.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import unicodedata\n",
    "# function to remove accented characters\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "# call function\n",
    "remove_accented_chars('Sómě Áccěntěd těxt. Some words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-chancellor",
   "metadata": {},
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developed-little",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you all I would contractions you are expanded do not think.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import contractions\n",
    "\n",
    "contractions.fix(\"Y’all i’d contractions you’re expanded don’t think.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-polls",
   "metadata": {},
   "source": [
    "### Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extreme-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'007 Not sure if this  was fun! 558923 What do you think of it.? 500USD!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import re\n",
    "# function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # define the pattern to keep\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pat, '', text)\n",
    " \n",
    "# call function\n",
    "remove_special_characters(\"007 Not sure@ if this % was #fun! 558923 What do# you think** of it.? $500USD!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-forum",
   "metadata": {},
   "source": [
    "### Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "literary-packet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Not sure if this  was fun!  What do you think of it.? USD!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import re\n",
    "# function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    # define the pattern to keep\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', text)\n",
    " \n",
    "# call function\n",
    "remove_numbers(\"007 Not sure@ if this % was #fun! 558923 What do# you think** of it.? $500USD!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-communist",
   "metadata": {},
   "source": [
    "### Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "diverse-ceiling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Article First sentence of some important article having lot of  punctuations And another one'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import string\n",
    "# function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    return text\n",
    "# call function\n",
    "remove_punctuation('Article: @First sentence of some, {important} article having lot of ~ punctuations. And another one;!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-affiliate",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handed-mercury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are eat and swim ; we have been eat and swim ; he eat and swim ; he ate and swam'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import nltk\n",
    "# function for stemming\n",
    "def get_stem(text):\n",
    "    stemmer = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "# call function\n",
    "get_stem(\"we are eating and swimming ; we have been eating and swimming ; he eats and swims ; he ate and swam \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-andrew",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supposed-assumption",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-32efc8dc7f15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# function to remove special characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_lem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sandbox\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, disable, exclude, config)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sandbox\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# function to remove special characters\n",
    "def get_lem(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "# call function\n",
    "get_lem(\"we are eating and swimming ; we have been eating and swimming ; he eats and swims ; he ate and swam \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-sector",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "# custom: removing words from list\n",
    "stopword_list.remove('not')\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    # convert sentence into token of words\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    # check in lowercase \n",
    "    t = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    text = ' '.join(t)    \n",
    "    return text\n",
    "# call function\n",
    "remove_stopwords(\"i am myself you the stopwords list and this article is not should removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-minimum",
   "metadata": {},
   "source": [
    "### Removing extra whitespaces and tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "# function to remove special characters\n",
    "def remove_extra_whitespace_tabs(text):\n",
    "    #pattern = r'^\\s+$|\\s+$'\n",
    "    pattern = r'^\\s*|\\s\\s*'\n",
    "    return re.sub(pattern, ' ', text).strip()\n",
    "# call function\n",
    "remove_extra_whitespace_tabs('  This web line  has \\t some extra  \\t   tabs and whitespaces  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-kuwait",
   "metadata": {},
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove special characters\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "# call function\n",
    "to_lowercase('ConVert THIS string to LOWER cASe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-arabic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
